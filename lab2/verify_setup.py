#!/usr/bin/env python3
"""
Quick verification script to check if the Lab 2 setup is working correctly.
This script tests all major components without running the full test suite.
"""

import os
import sys
import time
from typing import Dict, Any

def categorize_lean_error(error_msg: str) -> str:
    """More granular error categorization for better feedback"""
    if "type mismatch" in error_msg:
        return "TYPE_ERROR"
    elif "tactic failed" in error_msg:
        return "PROOF_TACTIC_ERROR"


def check_environment() -> Dict[str, Any]:
    """Check if the basic environment is set up correctly."""
    print("ğŸ” Checking environment setup...")
    
    results = {
        "python_version": sys.version,
        "working_directory": os.getcwd(),
        "lab2_directory": os.path.exists("src") and os.path.exists("tasks"),
        "lean_available": False,
        "api_key_present": bool(os.getenv("OPENAI_API_KEY"))
    }
    
    # Check if Lean is available
    try:
        import subprocess
        result = subprocess.run(["lake", "--version"], capture_output=True, text=True, timeout=10)
        results["lean_available"] = result.returncode == 0
        if results["lean_available"]:
            results["lean_version"] = result.stdout.strip()
    except Exception as e:
        results["lean_error"] = str(e)
    
    # Print results
    print(f"âœ… Python Version: {sys.version.split()[0]}")
    print(f"ğŸ“ Working Directory: {os.getcwd()}")
    print(f"ğŸ“‚ Lab2 Structure: {'âœ…' if results['lab2_directory'] else 'âŒ'}")
    print(f"ğŸ—ï¸  Lean Available: {'âœ…' if results['lean_available'] else 'âŒ'}")
    print(f"ğŸ”‘ API Key Present: {'âœ…' if results['api_key_present'] else 'âŒ'}")
    
    return results

def check_dependencies() -> Dict[str, Any]:
    """Check if required Python packages are installed."""
    print("\nğŸ” Checking Python dependencies...")
    
    required_packages = [
        "openai", "numpy", "sentence_transformers", 
        "tiktoken", "requests", "beautifulsoup4", "pydantic"
    ]
    
    results = {"missing_packages": [], "installed_packages": []}
    
    for package in required_packages:
        try:
            __import__(package.replace("-", "_"))
            results["installed_packages"].append(package)
            print(f"âœ… {package}")
        except ImportError:
            results["missing_packages"].append(package)
            print(f"âŒ {package}")
    
    return results

def check_lean_execution() -> Dict[str, Any]:
    """Test basic Lean execution."""
    print("\nğŸ” Testing Lean execution...")
    
    # Simple Lean code to test
    test_code = """
def test_function (x : Nat) : Nat := x

#check test_function
"""
    
    try:
        from src.lean_runner import execute_lean_code
        result = execute_lean_code(test_code)
        
        success = "executed successfully" in result
        print(f"ğŸ—ï¸  Lean Execution: {'âœ…' if success else 'âŒ'}")
        
        if not success and "Lean Error:" in result:
            error = result.split("Lean Error:")[1].strip()[:100]
            print(f"   Error: {error}...")
        
        return {"lean_execution": success, "result": result}
        
    except Exception as e:
        print(f"âŒ Lean execution failed: {e}")
        return {"lean_execution": False, "error": str(e)}

def check_database_creation() -> Dict[str, Any]:
    """Test database creation capability."""
    print("\nğŸ” Testing database creation...")
    
    results = {
        "openai_db_exists": os.path.exists("database.npy"),
        "mini_db_exists": os.path.exists("database_mini.npy"),
        "documents_exist": os.path.exists("documents") and len(os.listdir("documents")) > 0
    }
    
    print(f"ğŸ“Š OpenAI Database: {'âœ…' if results['openai_db_exists'] else 'âŒ'}")
    print(f"ğŸ“Š MiniLM Database: {'âœ…' if results['mini_db_exists'] else 'âŒ'}")
    print(f"ğŸ“š Documents Directory: {'âœ…' if results['documents_exist'] else 'âŒ'}")
    
    # Try to create database if none exist
    if not (results['openai_db_exists'] or results['mini_db_exists']):
        print("ğŸ”§ Attempting to create database...")
        try:
            import create_database
            success = create_database.create_database()
            results["database_creation"] = success
            print(f"ğŸ”§ Database Creation: {'âœ…' if success else 'âŒ'}")
        except Exception as e:
            results["database_creation_error"] = str(e)
            print(f"âŒ Database creation failed: {e}")
    
    return results

def test_single_task() -> Dict[str, Any]:
    """Test a single simple task end-to-end."""
    print("\nğŸ” Testing simple task (task_id_0)...")
    
    try:
        from src.main import main_workflow, get_problem_and_code_from_taskpath
        
        # Test the simplest task (identity function)
        task_path = "tasks/task_id_0"
        
        if not os.path.exists(task_path):
            return {"error": "Task directory not found"}
        
        # Get problem and template
        problem_description, lean_code_template = get_problem_and_code_from_taskpath(task_path)
        
        # Generate solution
        start_time = time.time()
        solution = main_workflow(problem_description, lean_code_template)
        runtime = time.time() - start_time
        
        # Check solution quality
        has_code = solution["code"] != "sorry" and len(solution["code"]) > 0
        has_proof = solution["proof"] != "sorry" and len(solution["proof"]) > 0
        
        print(f"âš¡ Runtime: {runtime:.2f} seconds")
        print(f"ğŸ’» Generated Code: {'âœ…' if has_code else 'âŒ'}")
        print(f"ğŸ“œ Generated Proof: {'âœ…' if has_proof else 'âŒ'}")
        
        # Test the generated solution
        try:
            from src.lean_runner import execute_lean_code
            
            full_code = lean_code_template.replace("{{code}}", solution["code"]).replace("{{proof}}", solution["proof"])
            test_result = execute_lean_code(full_code)
            
            success = "executed successfully" in test_result
            print(f"ğŸ§ª Solution Test: {'âœ…' if success else 'âŒ'}")
            
            return {
                "runtime": runtime,
                "has_code": has_code,
                "has_proof": has_proof,
                "solution_works": success,
                "test_result": test_result
            }
            
        except Exception as e:
            print(f"âŒ Solution testing failed: {e}")
            return {
                "runtime": runtime,
                "has_code": has_code,
                "has_proof": has_proof,
                "solution_works": False,
                "error": str(e)
            }
    
    except Exception as e:
        print(f"âŒ Task testing failed: {e}")
        return {"error": str(e)}

def check_agent_availability() -> Dict[str, Any]:
    """Check if AI agents are available."""
    print("\nğŸ” Testing agent availability...")
    
    results = {}
    
    try:
        from src.agents import create_agent_with_fallback
        
        # Test LLM agent
        llm_agent = create_agent_with_fallback("gpt-4o", "llm")
        llm_available = llm_agent.is_available() if hasattr(llm_agent, 'is_available') else True
        results["llm_agent"] = llm_available
        print(f"ğŸ¤– LLM Agent: {'âœ…' if llm_available else 'âŒ (fallback)'}")
        
        # Test reasoning agent
        reasoning_agent = create_agent_with_fallback("o3-mini", "reasoning")
        reasoning_available = reasoning_agent.is_available() if hasattr(reasoning_agent, 'is_available') else True
        results["reasoning_agent"] = reasoning_available
        print(f"ğŸ§  Reasoning Agent: {'âœ…' if reasoning_available else 'âŒ (fallback)'}")
        
    except Exception as e:
        print(f"âŒ Agent testing failed: {e}")
        results["error"] = str(e)
    
    return results

def generate_report(all_results: Dict[str, Any]) -> str:
    """Generate a summary report of all checks."""
    
    # Calculate overall health score
    checks = [
        all_results.get("environment", {}).get("lab2_directory", False),
        all_results.get("environment", {}).get("lean_available", False),
        all_results.get("dependencies", {}).get("missing_packages", []) == [],
        all_results.get("lean_execution", {}).get("lean_execution", False),
        all_results.get("task_test", {}).get("solution_works", False)
    ]
    
    health_score = sum(checks) / len(checks) * 100
    
    report = f"""
{'='*60}
LAB 2 SETUP VERIFICATION REPORT
{'='*60}

Overall Health Score: {health_score:.1f}%

ENVIRONMENT:
  âœ“ Lab2 Structure: {all_results.get("environment", {}).get("lab2_directory", False)}
  âœ“ Lean Available: {all_results.get("environment", {}).get("lean_available", False)}
  âœ“ API Key Present: {all_results.get("environment", {}).get("api_key_present", False)}

DEPENDENCIES:
  âœ“ Missing Packages: {len(all_results.get("dependencies", {}).get("missing_packages", []))}
  âœ“ Installed Packages: {len(all_results.get("dependencies", {}).get("installed_packages", []))}

FUNCTIONALITY:
  âœ“ Lean Execution: {all_results.get("lean_execution", {}).get("lean_execution", False)}
  âœ“ Database Available: {all_results.get("database", {}).get("openai_db_exists", False) or all_results.get("database", {}).get("mini_db_exists", False)}
  âœ“ Task Solution: {all_results.get("task_test", {}).get("solution_works", False)}

AGENTS:
  âœ“ LLM Agent: {all_results.get("agents", {}).get("llm_agent", False)}
  âœ“ Reasoning Agent: {all_results.get("agents", {}).get("reasoning_agent", False)}

RECOMMENDATIONS:
"""
    
    # Add specific recommendations
    if health_score >= 90:
        report += "  ğŸ‰ System is fully functional! Ready for submission.\n"
    elif health_score >= 75:
        report += "  âœ… System is mostly functional with minor issues.\n"
    elif health_score >= 50:
        report += "  âš ï¸  System has some issues but should work with fallbacks.\n"
    else:
        report += "  âŒ System needs attention before use.\n"
    
    # Specific fixes
    if not all_results.get("environment", {}).get("lean_available", False):
        report += "  â€¢ Install Lean 4: curl https://elan.lean-lang.org/elan-init.sh -sSf | sh\n"
    
    if all_results.get("dependencies", {}).get("missing_packages", []):
        report += "  â€¢ Install missing packages: pip install -r requirements.txt\n"
    
    if not (all_results.get("database", {}).get("openai_db_exists", False) or all_results.get("database", {}).get("mini_db_exists", False)):
        report += "  â€¢ Create database: python create_database.py\n"
    
    report += f"\n{'='*60}\n"
    
    return report

def main():
    """Run all verification checks."""
    print("ğŸš€ Starting Lab 2 Setup Verification")
    print("=" * 60)
    
    all_results = {}
    
    # Run all checks
    all_results["environment"] = check_environment()
    all_results["dependencies"] = check_dependencies()
    all_results["lean_execution"] = check_lean_execution()
    all_results["database"] = check_database_creation()
    all_results["agents"] = check_agent_availability()
    all_results["task_test"] = test_single_task()
    
    # Generate and print report
    report = generate_report(all_results)
    print(report)
    
    # Save results
    try:
        import json
        with open("verification_results.json", "w") as f:
            json.dump(all_results, f, indent=2, default=str)
        print("ğŸ“ Detailed results saved to verification_results.json")
    except Exception as e:
        print(f"âŒ Could not save results: {e}")
    
    return all_results

if __name__ == "__main__":
    results = main() 